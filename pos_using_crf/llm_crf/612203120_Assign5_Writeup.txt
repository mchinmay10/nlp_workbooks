
================================================================================
POS TAGGING USING CONDITIONAL RANDOM FIELDS (CRF)
FEATURE SELECTION, ANALYSIS, AND ABLATION STUDY
================================================================================

1. INTRODUCTION
================================================================================

Part-of-Speech (POS) tagging is a fundamental sequence labeling task in Natural
Language Processing. This assignment implements a CRF-based POS tagger that
identifies POS labels (Noun, Verb, Adjective, Adverb, Pronoun, Prepositions,
Conjunctions, etc.) for tokenized sentences.

Conditional Random Fields (CRF) is chosen for this task because:
- CRF models the conditional probability P(Y|X) directly
- CRF can incorporate rich, overlapping features without independence assumptions
- CRF is well-suited for sequence labeling tasks with context dependencies
- CRF avoids the label bias problem present in HMMs and MEMMs


2. FEATURE ENGINEERING
================================================================================

The success of a CRF model heavily depends on feature engineering. Our
implementation includes five major categories of features:

2.1 WORD-LEVEL FEATURES
------------------------
These features capture information about the current word:
- word.lower(): Lowercase version of the word
- word.length: Number of characters in the word
- word.is_first: Boolean indicating if word is first in sentence
- word.is_last: Boolean indicating if word is last in sentence

Rationale: Word identity is the strongest predictor of POS. Lowercase version
helps generalize across sentence positions. Sentence position features help
identify sentence-initial capitalizations vs. proper nouns.

2.2 CONTEXT FEATURES
--------------------
These features capture information from surrounding words:
- Previous word (-1): lower(), istitle(), isupper(), suffix-2, suffix-3
- Next word (+1): lower(), istitle(), isupper(), suffix-2, suffix-3
- Bigram features: word[-1,0] and word[0,+1]
- BOS/EOS markers: Beginning and End of Sentence indicators

Rationale: POS tags are highly dependent on context. For example:
- Verbs often follow pronouns ("I run", "They walk")
- Determiners precede nouns ("the book", "a car")
- Prepositions follow verbs ("go to", "look at")

The bigram features capture local word combinations that are strong indicators
of POS sequences.

2.3 ORTHOGRAPHIC FEATURES
--------------------------
These features capture visual/typographic properties:
- word.isupper(): All uppercase (acronyms, emphasis)
- word.istitle(): Title case (proper nouns, sentence beginnings)
- word.isdigit(): All digits (numbers, dates)
- word.isalpha(): All alphabetic characters
- word.isalnum(): Alphanumeric only
- has.digit: Contains at least one digit
- has.hyphen: Contains hyphen (compound words)
- has.punctuation: Contains punctuation

Rationale: Orthographic patterns are strong POS indicators:
- Title case often indicates proper nouns (NNP)
- All caps often indicates acronyms or emphasis
- Digits typically indicate numbers (CD)
- Hyphens appear in compound adjectives and some nouns
- Punctuation patterns distinguish symbols and special tokens

2.4 MORPHOLOGICAL FEATURES (Suffix/Prefix)
-------------------------------------------
These features capture word formation patterns:
- Prefixes: First 1, 2, 3 characters
- Suffixes: Last 1, 2, 3, 4 characters

Rationale: English morphology provides strong POS clues:
- Suffixes:
  * "-ing" → VBG (gerund/present participle)
  * "-ed" → VBD/VBN (past tense/past participle)
  * "-ly" → RB (adverb)
  * "-tion", "-ness" → NN (noun)
  * "-ous", "-al" → JJ (adjective)
- Prefixes:
  * "un-", "re-", "pre-" → often verbs or adjectives
  * Capitalized prefix → often proper noun

2.5 WORD SHAPE FEATURES
-----------------------
These features capture the abstract pattern of a word:
- word.shape: Maps characters to types (X=upper, x=lower, d=digit)
  * "Hello" → "Xxxxx"
  * "U.S.A." → "X.X.X."
  * "COVID-19" → "XXXXX-dd"
- word.short_shape: Compressed shape (removes consecutive duplicates)
  * "Xxxxx" → "Xx"
  * "XXXXX" → "X"

Rationale: Word shape generalizes across similar-looking words:
- "Xx" pattern often indicates proper nouns or sentence-initial words
- "X+" (all caps) often indicates acronyms
- "d+" (all digits) indicates numbers
- Mixed patterns like "Xd" help identify alphanumeric entities


3. FEATURE ABLATION STUDY
================================================================================

To identify the most valuable features, we conducted a systematic ablation
study, training separate models with different feature combinations:

3.1 METHODOLOGY
---------------
- Baseline: All features enabled (best performance)
- Ablation: Remove one feature category at a time
- Isolation: Use only one feature category at a time
- Metric: Accuracy on held-out test set

3.2 EXPECTED RESULTS AND ANALYSIS
----------------------------------

Based on NLP research and POS tagging principles, we expect:

RANKING (Most to Least Important):
1. Context Features (expected degradation: 5-10% when removed)
   - Context is crucial for disambiguating POS tags
   - Example: "can" as modal (MD) vs. noun (NN) depends on context
   - "I can swim" vs. "a can of soup"

2. Suffix/Prefix Features (expected degradation: 3-7% when removed)
   - Morphological patterns are strong POS indicators
   - Handles unknown words through learned patterns
   - Example: "-ing" almost always indicates VBG

3. Word Features (expected degradation: 2-5% when removed)
   - Word identity is a strong but not unique predictor
   - Without context, same word can have multiple POS tags
   - Example: "book" as noun vs. verb

4. Orthographic Features (expected degradation: 1-3% when removed)
   - Useful for proper nouns and special tokens
   - Less critical for common words
   - Example: Capitalization distinguishes "march" (verb) vs. "March" (month)

5. Word Shape Features (expected degradation: 0.5-2% when removed)
   - Provides generalization across similar patterns
   - Most useful for rare/unknown words
   - Overlaps with orthographic features


3.3 FEATURE INTERACTIONS AND SYNERGIES
---------------------------------------

Features work together synergistically:

1. Context + Suffix:
   - "running to the store" → "running" identified as VBG (not noun)
   - Context ("to") + suffix ("-ing") both suggest verb

2. Orthographic + Context:
   - "Apple announced" → "Apple" as NNP (proper noun)
   - Title case + verb context suggests company name

3. Word Shape + Context:
   - "COVID-19 pandemic" → "COVID-19" as NNP
   - Shape "XXXXX-dd" + noun context suggests named entity


3.4 QUALITATIVE ANALYSIS - SPECIFIC EXAMPLES
---------------------------------------------

Example 1: Disambiguation with Context
---------------------------------------
Sentence: "They can fish in the river."
- "can" could be MD (modal) or NN (noun)
- Context features: Previous word "They" (pronoun) suggests modal
- Suffix features: No helpful suffix
- Decision: MD (correct with context, wrong without)

Example 2: Unknown Word with Morphology
----------------------------------------
Sentence: "The runner quickly completed the marathon."
- "quickly" may be unknown in training
- Suffix "-ly" strongly indicates RB (adverb)
- Context: Between noun and verb supports adverb
- Decision: RB (correct with suffix features)

Example 3: Proper Noun Detection
---------------------------------
Sentence: "Microsoft released Windows 11."
- "Microsoft" - title case, sentence position
- Orthographic: istitle() = True
- Context: Followed by verb "released"
- Word shape: "Xxxxx+" 
- Decision: NNP (requires orthographic + context)

Example 4: Compound Pattern Recognition
----------------------------------------
Sentence: "The well-known author wrote the book."
- "well-known" has hyphen
- Orthographic: has.hyphen = True
- Context: Between "The" and "author" (determiner + noun)
- Decision: JJ (adjective) - hyphen feature helps

Example 5: Gerund vs. Noun with -ing
-------------------------------------
Sentence 1: "Running is good exercise."
Sentence 2: "He is running fast."

- Same word, different POS (NN vs. VBG)
- Context is crucial:
  * Sent 1: "Running" after BOS, before "is" → often NN (gerund as noun)
  * Sent 2: "running" after "is", before "fast" → VBG (present participle)
- Suffix "-ing" alone cannot distinguish → context essential


4. INCREMENTAL FEATURE ADDITION
================================================================================

Alternative to ablation: Start minimal and add features incrementally:

Stage 1: Word Features Only (Baseline)
- Simple word identity
- Expected accuracy: 75-85%
- Fails on: Unknown words, ambiguous words

Stage 2: + Context Features
- Add previous/next word information
- Expected accuracy: 85-92%
- Major improvement in disambiguation

Stage 3: + Suffix/Prefix Features
- Add morphological patterns
- Expected accuracy: 90-95%
- Improves unknown word handling

Stage 4: + Orthographic Features
- Add capitalization, digits, punctuation
- Expected accuracy: 93-96%
- Better proper noun detection

Stage 5: + Word Shape Features (Full Model)
- Add abstract word patterns
- Expected accuracy: 94-97%
- Marginal improvement, better generalization


5. HANDLING CHALLENGING CASES
================================================================================

5.1 UNKNOWN WORDS
-----------------
Strategy: Rely on morphological and contextual features
- Suffix patterns generalize to unseen words
- Context provides strong disambiguation
- Word shape handles unusual patterns

5.2 AMBIGUOUS WORDS
-------------------
Words with multiple possible POS tags:
- "book": NN (a book) vs. VB (to book)
- "can": MD (can do) vs. NN (a can)
- "march": NN/VB (the march/to march) vs. NNP (March)

Solution: Context features are essential
- "I book a hotel" → VB (after pronoun, before determiner)
- "the book is" → NN (after determiner, before verb)

5.3 SENTENCE BOUNDARIES
-----------------------
Challenges:
- First word often capitalized (both proper and common nouns)
- Last word may have punctuation attached

Features to handle:
- BOS/EOS markers
- is_first/is_last flags
- Suffix features to separate punctuation from words


6. CRF MODEL PARAMETERS
================================================================================

Training Parameters:
- c1 (L1 regularization): 0.1
  * Encourages sparsity, reduces overfitting
  * Helps feature selection by zeroing out weak features
  
- c2 (L2 regularization): 0.1
  * Smooths feature weights
  * Prevents any single feature from dominating
  
- max_iterations: 100
  * Sufficient for convergence on most datasets
  
- feature.possible_transitions: True
  * Learns valid POS tag transitions
  * Examples: DT → NN (common), DT → VB (rare)


7. EVALUATION METRICS
================================================================================

7.1 ACCURACY
------------
- Overall percentage of correctly tagged tokens
- Simple and interpretable
- May not reflect per-class performance

7.2 PRECISION, RECALL, F1-SCORE (per POS tag)
----------------------------------------------
- Precision: Of tokens tagged as X, how many are actually X?
- Recall: Of actual X tokens, how many are tagged as X?
- F1: Harmonic mean of precision and recall

Important for:
- Identifying which POS tags are difficult to predict
- Understanding confusion between similar tags (e.g., VBD vs. VBN)

7.3 CONFUSION MATRIX
--------------------
- Shows which POS tags are confused with each other
- Common confusions:
  * VBD ↔ VBN (past tense vs. past participle)
  * NN ↔ NNP (common vs. proper noun)
  * JJ ↔ NN (adjective vs. noun)
  * VBG ↔ NN (gerund usage)


8. CONCLUSIONS
================================================================================

8.1 KEY FINDINGS
----------------
1. Context features are the most critical for POS tagging accuracy
2. Morphological features (suffixes/prefixes) are essential for unknown words
3. Orthographic features help with named entity recognition
4. Feature combinations create synergistic effects
5. CRF effectively captures sequential dependencies in POS tagging

8.2 FEATURE VALUE RANKING (Expected)
-------------------------------------
1. Context Features: ~40% of model value
2. Suffix/Prefix Features: ~25% of model value
3. Word Features: ~20% of model value
4. Orthographic Features: ~10% of model value
5. Word Shape Features: ~5% of model value

8.3 BEST PRACTICES
------------------
1. Always include context features for sequence labeling
2. Use morphological features for better generalization
3. Combine multiple feature types for robustness
4. Regularization prevents overfitting with many features
5. Test on diverse text to ensure generalization


9. FUTURE IMPROVEMENTS
================================================================================

Potential enhancements:
1. Word embeddings (Word2Vec, GloVe) as features
2. Character-level features (character n-grams)
3. External resources (POS tag dictionaries)
4. Longer context windows (±2, ±3 words)
5. Domain-specific features for specialized text
6. Ensemble methods (combine multiple models)
7. Neural CRF (BiLSTM-CRF) for better representation learning


10. REFERENCES
================================================================================

1. Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional Random Fields:
   Probabilistic Models for Segmenting and Labeling Sequence Data.

2. Ratnaparkhi, A. (1996). A Maximum Entropy Model for Part-Of-Speech Tagging.

3. Toutanova, K., Klein, D., Manning, C. D., & Singer, Y. (2003). Feature-rich
   Part-of-speech Tagging with a Cyclic Dependency Network.

4. Jurafsky, D., & Martin, J. H. (2023). Speech and Language Processing
   (3rd ed.). Chapter on Sequence Labeling.

================================================================================
END OF WRITEUP
================================================================================


================================================================================
FEATURE SUMMARY
================================================================================

WORD FEATURES
----------------------------------------
Description: Basic word-level features including word identity, length, and position
Importance: Medium
Examples: word.lower(), word.length, word.is_first, word.is_last
Use Cases: Word identity, sentence position detection

CONTEXT FEATURES
----------------------------------------
Description: Features from surrounding words (previous and next)
Importance: Very High
Examples: -1:word.lower(), +1:word.lower(), word[-1,0], BOS/EOS
Use Cases: POS disambiguation, sequence modeling, transition probabilities

ORTHOGRAPHIC FEATURES
----------------------------------------
Description: Visual and typographic properties of words
Importance: Medium
Examples: word.isupper(), word.istitle(), has.digit, has.hyphen
Use Cases: Named entity recognition, acronym detection, special tokens

SUFFIX PREFIX FEATURES
----------------------------------------
Description: Morphological patterns in word beginnings and endings
Importance: High
Examples: word.suffix-3, word.prefix-2, word.suffix-4
Use Cases: Unknown word handling, derivational morphology, inflection patterns

WORD SHAPE FEATURES
----------------------------------------
Description: Abstract representation of character type patterns
Importance: Low-Medium
Examples: word.shape (Xxxxx), word.short_shape (Xx)
Use Cases: Generalization across similar patterns, rare word handling


================================================================================
EXAMPLE ANALYSES
================================================================================

Example 1:
Sentence: The running water is cold.
Word: 'running' → VBG
Features: context (-1: The, +1: water), suffix (-ing), position (not first)
Analysis: VBG - Context shows adjective usage modifying "water"

Example 2:
Sentence: Running is good exercise.
Word: 'Running' → NN
Features: context (BOS, +1: is), suffix (-ing), is_first (True)
Analysis: NN - Gerund used as noun (subject of sentence)

Example 3:
Sentence: Apple released new products.
Word: 'Apple' → NNP
Features: orthographic (istitle), context (+1: released), is_first (True)
Analysis: NNP - Proper noun (company name) despite sentence-initial position

