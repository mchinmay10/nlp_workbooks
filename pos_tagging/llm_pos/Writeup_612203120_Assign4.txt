Part of Speech (POS) Tagging using Hidden Markov Model and Viterbi Algorithm
==========================================================================

This writeup explains the implementation of a Part of Speech (POS) tagger using a Hidden Markov Model (HMM) with the Viterbi algorithm. The implementation includes features like Laplace smoothing and unknown word handling.

1. Code Structure and Components
------------------------------
The code is organized into several key functions:

a) read_labeled_corpus():
   - Reads the training corpus where words are labeled with POS tags in format "word_tag"
   - Returns a list of sentences (each containing word-tag pairs) and a sorted list of unique tags

b) create_transition_matrix():
   - Creates the transition probability matrix for tag-to-tag transitions
   - Implements Laplace smoothing using parameter alpha
   - Handles probabilities in log space to prevent underflow

c) create_emission_matrix():
   - Creates the emission probability matrix for tag-to-word emissions
   - Implements Laplace smoothing
   - Includes handling for unknown words with "<UNK>" token

d) assign_unk():
   - Implements a rule-based approach for handling unknown words
   - Uses suffix and capitalization patterns to guess POS tags
   - Rules include:
     * Words ending in 'ing' → VBG (Gerund)
     * Words ending in 'ed' → VBD (Past tense verb)
     * Words ending in 'ly' → RB (Adverb)
     * Capitalized words → NNP (Proper noun)
     * Default case → NN (Common noun)

e) train_hmm():
   - Main training function that builds the HMM model
   - Calculates transition and emission probabilities
   - Uses special tags <s> and </s> for sentence boundaries
   - Implements Laplace smoothing with configurable alpha parameter

f) viterbi():
   - Implements the Viterbi algorithm for sequence decoding
   - Works in log space to prevent numerical underflow
   - Includes special handling for unknown words using assign_unk()
   - Returns the most likely sequence of POS tags for given words

2. Key Features and Improvements
------------------------------
1. Laplace Smoothing:
   - Implemented to handle unseen transitions and emissions
   - Configurable smoothing parameter (alpha)
   - Prevents zero probabilities in the model

2. Unknown Word Handling:
   - Two-level approach:
     a) Generic "<UNK>" token with smoothed probabilities
     b) Rule-based suffix analysis for better tag prediction

3. Log Space Calculations:
   - Prevents numerical underflow in probability calculations
   - Uses log probabilities throughout the Viterbi algorithm

4. Command Line Interface:
   - Flexible input/output specification
   - Configurable smoothing parameter
   - Easy to use with different datasets

3. Usage
-------
The program can be run from command line with the following arguments:
- --train: Path to training file
- --test: Path to test file
- --output: Path for output file
- --alpha: Smoothing parameter (default=1.0)

Example:
python hmm_viterbi.py --train train.txt --test test.txt --output tagged.txt --alpha 1.0

4. Implementation Details
----------------------
- Language: Python 3
- Key Libraries: math, collections (defaultdict, Counter)
- Data Structures: 
  * defaultdict for sparse matrices
  * Counter for frequency counting
  * Lists and dictionaries for dynamic programming

The implementation focuses on efficiency and robustness while maintaining readability and modularity. The code includes comprehensive error handling and follows Python best practices.
